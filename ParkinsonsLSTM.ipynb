{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ParkinsonsLSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCusvOKVOCxG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d20d53c-47fb-4f9a-a9ea-d6e8727d83af"
      },
      "source": [
        "import librosa\n",
        "import os\n",
        "import glob\n",
        "import IPython.display as ipd\n",
        "from pathlib import Path\n",
        "import timeit\n",
        "import time, sys\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "\n",
        "%tensorflow_version 1.x #version 1 works without problems\n",
        "import tensorflow\n",
        "\n",
        "from tensorflow.keras import models\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "\n",
        "import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense, BatchNormalization, Activation, GaussianNoise, LSTM\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.x #version 1 works without problems`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE0YusphooQh",
        "outputId": "427e394a-bdda-484a-9a4b-8069532256de"
      },
      "source": [
        "DATA_DIR = Path('/content/drive/MyDrive/PhD_Project_Experiments/Spontaneous_Dialogue_PD_Dataset') \n",
        "diagnosis = [x.name for x in DATA_DIR.glob('*') if x.is_dir()]\n",
        "diagnosis"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Parkinsons_Disease', 'Healthy_Control']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrTl9OoLotYv"
      },
      "source": [
        "def create_paths_ds(paths: Path, label: str) -> list:\n",
        "    EXTENSION_TYPE = '.wav'\n",
        "    return [(x, label) for x in paths.glob('*' + EXTENSION_TYPE)]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQeNlQmrvFpV",
        "outputId": "68fb7bd5-89be-49d8-df47-c36c88a62884"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "categories_to_use = [\n",
        "    'Parkinsons_Disease',\n",
        "    'Healthy_Control',\n",
        "]\n",
        "\n",
        "NUM_CLASSES = len(categories_to_use)\n",
        "\n",
        "print(f'Number of classes: {NUM_CLASSES}')\n",
        "\n",
        "paths_all_labels = []\n",
        "for cat in categories_to_use:\n",
        "    paths_all_labels += create_paths_ds(DATA_DIR / cat, cat)\n",
        " \n",
        "X_train, X_test = train_test_split(paths_all_labels,test_size=0.1, stratify = [paths_all_labels[y][1] for y in range(len(paths_all_labels))] ) #fix stratified sampling for test data\n",
        "X_train, X_val = train_test_split(X_train, test_size=0.2, stratify = [X_train[y][1] for y in range(len(X_train))] ) \n",
        "\n",
        "for i in categories_to_use:\n",
        "  print('Number of train samples for '+i+': '+ str([X_train[y][1] for y in range(len(X_train))].count(i))) #checks whether train samples are equally divided\n",
        "  print('Number of test samples for '+i+': '+ str([X_test[y][1] for y in range(len(X_test))].count(i))) #checks whether test samples are equally divided\n",
        "  print('Number of validation samples for '+i+': '+ str([X_val[y][1] for y in range(len(X_val))].count(i))) #checks whether val samples are equally divided\n",
        "\n",
        "print(f'Train length: {len(X_train)}')\n",
        "print(f'Validation length: {len(X_val)}')\n",
        "print(f'Test length: {len(X_test)}')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of classes: 2\n",
            "Number of train samples for Parkinsons_Disease: 10\n",
            "Number of test samples for Parkinsons_Disease: 2\n",
            "Number of validation samples for Parkinsons_Disease: 3\n",
            "Number of train samples for Healthy_Control: 15\n",
            "Number of test samples for Healthy_Control: 2\n",
            "Number of validation samples for Healthy_Control: 4\n",
            "Train length: 25\n",
            "Validation length: 7\n",
            "Test length: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UozezauvXrc"
      },
      "source": [
        "def load_and_preprocess_cnn(dataset, SAMPLE_SIZE = 30):\n",
        "    IMG_SIZE = (216,128) \n",
        "    progress = 0\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "    for (path, label) in dataset:\n",
        "        audio, sr = librosa.load(path)\n",
        "        dur = librosa.get_duration(audio, sr = sr)\n",
        "        sampleNum = int(dur / SAMPLE_SIZE)\n",
        "        offset = (dur % SAMPLE_SIZE) / 2\n",
        "        for i in range(sampleNum):\n",
        "            audio, sr = librosa.load(path, offset= offset+i, duration=SAMPLE_SIZE)\n",
        "            sample = librosa.feature.melspectrogram(audio, sr=sr)\n",
        "            sample = cv2.resize(sample, dsize=IMG_SIZE)\n",
        "            sample = np.expand_dims(sample,-1)\n",
        "            data += [(sample, label)]\n",
        "            labels += [label]\n",
        "\n",
        "        progress +=1\n",
        "        print('\\r Progress: '+str(round(100*progress/len(dataset))) + '%', end='')\n",
        "    return data, labels"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WByjgq7PvXla"
      },
      "source": [
        "def load_and_preprocess_lstm(dataset, SAMPLE_SIZE = 30):\n",
        "    IMG_SIZE = (216,128) \n",
        "    progress=0\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "    for (path, label) in dataset:\n",
        "        audio, sr = librosa.load(path)\n",
        "        dur = librosa.get_duration(audio, sr = sr)\n",
        "        sampleNum = int(dur / SAMPLE_SIZE)\n",
        "        offset = (dur % SAMPLE_SIZE) / 2\n",
        "        for i in range(sampleNum):\n",
        "            audio, sr = librosa.load(path, offset= offset+i, duration=SAMPLE_SIZE)\n",
        "            sample = librosa.feature.melspectrogram(audio, sr=sr)\n",
        "            # print(sample.shape)\n",
        "            sample = cv2.resize(sample, dsize=IMG_SIZE)\n",
        "            sample = np.expand_dims(sample,-1)\n",
        "            data += [(sample, label)]\n",
        "            labels += [label]\n",
        "        progress +=1\n",
        "        print('\\r Progress: '+str(round(100*progress/len(dataset))) + '%', end='')\n",
        "    return data, labels"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaFLFsl8vgKj"
      },
      "source": [
        "def retrieve_samples(sample_size, model_type):\n",
        "\n",
        "    if model_type == 'cnn':\n",
        "  \n",
        "        print(\"\\nLoading train samples\")\n",
        "        X_train_samples, train_labels = load_and_preprocess_cnn(X_train,sample_size)\n",
        "        print(\"\\nLoading test samples\")\n",
        "        X_test_samples, test_labels = load_and_preprocess_cnn(X_test,sample_size)\n",
        "        print(\"\\nLoading val samples\")\n",
        "        X_val_samples, val_labels = load_and_preprocess_cnn(X_val,sample_size)\n",
        "        print('\\n')\n",
        "\n",
        "    elif model_type == 'lstm':\n",
        "\n",
        "        print(\"\\nLoading train samples\")\n",
        "        X_train_samples, train_labels = load_and_preprocess_lstm(X_train,sample_size)\n",
        "        print(\"\\nLoading test samples\")\n",
        "        X_test_samples, test_labels = load_and_preprocess_lstm(X_test,sample_size)\n",
        "        print(\"\\nLoading val samples\")\n",
        "        X_val_samples, val_labels = load_and_preprocess_lstm(X_val,sample_size)      \n",
        "        print('\\n')\n",
        "\n",
        "    elif model_type == \"cnnlstm\":\n",
        "\n",
        "        print(\"\\nLoading train samples\")\n",
        "        X_train_samples, train_labels = load_and_preprocess_lstm(X_train,sample_size)\n",
        "        print(\"\\nLoading test samples\")\n",
        "        X_test_samples, test_labels = load_and_preprocess_lstm(X_test,sample_size)\n",
        "        print(\"\\nLoading val samples\")\n",
        "        X_val_samples, val_labels = load_and_preprocess_lstm(X_val,sample_size)      \n",
        "        print('\\n')\n",
        "\n",
        "    print(\"shape: \" + str(X_train_samples[0][0].shape))\n",
        "    print(\"number of training samples: \"+ str(len(X_train_samples)))\n",
        "    print(\"number of validation samples: \"+ str(len(X_val_samples)))\n",
        "    print(\"number of test samples: \"+ str(len(X_test_samples)))\n",
        "\n",
        "\n",
        "    return X_train_samples, X_test_samples, X_val_samples"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsFDZpbwvnRh"
      },
      "source": [
        "def create_cnn_model(input_shape):\n",
        "  # input_shape = X_train_samples[0][0].shape\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "  model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(GaussianNoise(0.1))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(NUM_CLASSES, activation='sigmoid')) #Compile\n",
        "  model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.adam(), metrics=['accuracy'])\n",
        "  print(model.summary())\n",
        "  return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X941kHnkvsP3"
      },
      "source": [
        "def create_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(LSTM(units = 512, dropout=0.5, recurrent_dropout=0.3, return_sequences = True, input_shape = input_shape))\n",
        "    model.add(LSTM(units = 512, dropout=0.5, recurrent_dropout=0.3, return_sequences = False))\n",
        "    model.add(Dense(units=NUM_CLASSES, activation='sigmoid'))#Compile\n",
        "\n",
        "    model.compile(loss=tensorflow.keras.losses.binary_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "\n",
        "    return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9xe18RmZidw"
      },
      "source": [
        "def create_cnn_lstm_model(input_shape):\n",
        "\n",
        "    model = Sequential()\n",
        "    cnn = tensorflow.keras.applications.DenseNet169(include_top=True, weights=None, input_tensor=None, input_shape=input_shape, pooling=None, classes=2)\n",
        "    # define LSTM model\n",
        "    model.add(tensorflow.keras.layers.TimeDistributed(cnn, input_shape=input_shape))\n",
        "    model.add(LSTM(units = 512, dropout=0.5, recurrent_dropout=0.3, return_sequences = True, input_shape = input_shape))\n",
        "    model.add(LSTM(units = 512, dropout=0.5, recurrent_dropout=0.3, return_sequences = False))\n",
        "    model.add(Dense(units=NUM_CLASSES, activation='sigmoid'))#Compile\n",
        "\n",
        "    model.compile(loss=tensorflow.keras.losses.binary_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "\n",
        "    return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxzMz2TTvwN9"
      },
      "source": [
        "def create_model_data_and_labels(X_train_samples, X_val_samples, X_test_samples):\n",
        "    #Prepare samples to work for training the model\n",
        "    labelizer = LabelEncoder()\n",
        "\n",
        "    #prepare training data and labels\n",
        "    x_train = np.array([x[0] for x in X_train_samples])\n",
        "    y_train = np.array([x[1] for x in X_train_samples])\n",
        "    y_train = labelizer.fit_transform(y_train) \n",
        "    y_train = to_categorical(y_train)\n",
        "    print(y_train.shape)\n",
        "\n",
        "    #prepare validation data and labels\n",
        "    x_val = np.array([x[0] for x in X_val_samples])\n",
        "    y_val = np.array([x[1] for x in X_val_samples])\n",
        "    y_val = labelizer.transform(y_val)\n",
        "    y_val = to_categorical(y_val)\n",
        "\n",
        "    #prepare test data and labels\n",
        "    x_test = np.array([x[0] for x in X_test_samples])\n",
        "    y_test = np.array([x[1] for x in X_test_samples])\n",
        "    y_test = labelizer.transform(y_test)\n",
        "    y_test = to_categorical(y_test)\n",
        "\n",
        "    return x_train, y_train, x_val, y_val, x_test, y_test, labelizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf5Xywkgvzcs"
      },
      "source": [
        "def plot_data(history, ss):\n",
        "    # list all data in history\n",
        "    print(history.history.keys())\n",
        "    # summarize history for accuracy\n",
        "    sns.set(font_scale=1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "    plt.savefig('model_accuracy_size_'+str(ss)+'.png')\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    plt.show()\n",
        "    plt.savefig('model_loss_size_'+str(ss)+'.png')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEZlxXXfv-ja"
      },
      "source": [
        "def plot_conf_matrix(predictions, y_test, ss):\n",
        "    sns.set(font_scale=2.5)\n",
        "    matrix = confusion_matrix(labelizer.inverse_transform(y_test.argmax(axis=1)), \n",
        "                          labelizer.inverse_transform(predictions.argmax(axis=1)), \n",
        "                         labels=labelizer.classes_\n",
        "                         )\n",
        "\n",
        "    matrix = pd.DataFrame(matrix, index=labelizer.classes_, columns=labelizer.classes_)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "   \n",
        "    sns.heatmap(matrix, annot=True, fmt='.2f', linewidth=5, cmap=\"Greens\")\n",
        "    ax.set_title('Confusion Matrix for (mis)classifications \\n with fragment size: '+str(ss)+ ' seconds')\n",
        "    ax.set_xlabel('Predictions')\n",
        "    ax.set_ylabel('True values');\n",
        "\n",
        "    fig.savefig('conf_matrix_size_'+str(ss)+'.png', bbox_inches='tight')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "54STgQ6twBUv",
        "outputId": "7935c461-4d86-4f3e-a9f6-01b5d6e369e3"
      },
      "source": [
        "#Main loop for testing multiple sample sizes\n",
        "\n",
        "#choose model type: 'cnn' or 'lstm'\n",
        "model_type = 'cnnlstm'\n",
        "\n",
        "n_epochs = 20\n",
        "patience= 20\n",
        "es = EarlyStopping(patience=20)\n",
        "fragment_sizes = [5,10]\n",
        "start = timeit.default_timer()\n",
        "\n",
        "ModelData = pd.DataFrame(columns = ['Model Type','Fragment size (s)', 'Time to Compute (s)',  'Early Stopping epoch', 'Training accuracy', 'Validation accuracy', 'Test Accuracy']) #create a DataFrame for storing the results \n",
        "\n",
        "conf_matrix_data = []\n",
        "\n",
        "for i in fragment_sizes:\n",
        "\n",
        "    start_per_size = timeit.default_timer()\n",
        "\n",
        "    print(f'\\n---------- Model trained on fragments of size: {i} seconds ----------------')\n",
        "    X_train_samples, X_test_samples, X_val_samples = retrieve_samples(i,model_type)\n",
        "    x_train, y_train, x_val, y_val, x_test, y_test, labelizer = create_model_data_and_labels(X_train_samples, X_val_samples, X_test_samples)\n",
        "    print(\"x_train shape is:......\")\n",
        "    print(x_train.shape)\n",
        "    print(\"y_train shape is:......\")\n",
        "    print(y_train.shape)\n",
        "    print(\"input shape is:......\")\n",
        "    print(X_train_samples[0][0].shape)\n",
        "\n",
        "    if model_type == 'cnn':\n",
        "        model = create_cnn_model(X_train_samples[0][0].shape)\n",
        "    elif model_type == 'lstm':\n",
        "        model = create_lstm_model(X_train_samples[0][0].shape)\n",
        "    elif model_type == 'cnnlstm':\n",
        "        model = create_cnn_lstm_model(X_train_samples[0][0].shape)\n",
        "\n",
        "\n",
        "    history = model.fit(x_train, y_train, \n",
        "              batch_size = 8, \n",
        "              epochs=n_epochs,\n",
        "              verbose=1, \n",
        "              callbacks=[es],\n",
        "              validation_data=(x_val, y_val))\n",
        "    print('Finished training')\n",
        "\n",
        "\n",
        "    early_stopping_epoch = len(history.history['accuracy'])\n",
        "    training_accuracy = history.history['accuracy'][early_stopping_epoch-1-patience]\n",
        "    validation_accuracy = history.history['val_accuracy'][early_stopping_epoch-1-patience]\n",
        "\n",
        "    plot_data(history, i)\n",
        "\n",
        "    predictions = model.predict(x_test)\n",
        "    score = accuracy_score(labelizer.inverse_transform(y_test.argmax(axis=1)), labelizer.inverse_transform(predictions.argmax(axis=1)))\n",
        "\n",
        "    print('Fragment size = ' + str(i) + ' seconds')\n",
        "    print('Accuracy on test samples: ' + str(score))\n",
        "    \n",
        "    conf_matrix_data += [(predictions, y_test, i)]\n",
        "\n",
        "    stop_per_size = timeit.default_timer()\n",
        "    time_to_compute = round(stop_per_size - start_per_size)\n",
        "\n",
        "    print ('Time to compute: '+str(time_to_compute))\n",
        "\n",
        "    ModelData.loc[len(ModelData)] = [model_type, i, time_to_compute, early_stopping_epoch, training_accuracy, validation_accuracy, score] #store particular settings configuration, early stoppping epoch and accuracies in dataframe\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "print ('\\ntime to compute: '+str(stop-start))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---------- Model trained on fragments of size: 5 seconds ----------------\n",
            "\n",
            "Loading train samples\n",
            " Progress: 100%\n",
            "Loading test samples\n",
            " Progress: 100%\n",
            "Loading val samples\n",
            " Progress: 100%\n",
            "\n",
            "shape: (128, 216, 1)\n",
            "number of training samples: 318\n",
            "number of validation samples: 94\n",
            "number of test samples: 72\n",
            "(318, 2)\n",
            "x_train shape is:......\n",
            "(318, 128, 216, 1)\n",
            "y_train shape is:......\n",
            "(318, 2)\n",
            "input shape is:......\n",
            "(128, 216, 1)\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-1513955f9219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_lstm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cnnlstm'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_cnn_lstm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-5ddc246f163f>\u001b[0m in \u001b[0;36mcreate_cnn_lstm_model\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDenseNet169\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# define LSTM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    175\u001b[0m           \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m           \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m           \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m           \u001b[0mset_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0minner_mask_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_mask_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m       \u001b[0;31m# Shape: (num_samples, timesteps, ...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    693\u001b[0m                                 ' implement a `call` method.')\n\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;31m# are casted, not before.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         input_spec.assert_input_compatibility(self.input_spec, inputs,\n\u001b[0;32m--> 819\u001b[0;31m                                               self.name)\n\u001b[0m\u001b[1;32m    820\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    175\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                          str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer zero_padding2d is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [None, 216, 1]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwyCeJQHOREj"
      },
      "source": [
        "print(ModelData)\n",
        "\n",
        "ModelData.head()\n",
        "\n",
        "import os.path\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "filenumber=1\n",
        "while os.path.isfile('ModelData'+str(filenumber)+'.csv'): #find the first free file\n",
        "    filenumber += 1\n",
        "\n",
        "ModelData.to_csv(r'ModelData'+str(filenumber)+'.csv') #store data in the first free csv file\n",
        "# files.download('ModelData'+str(filenumber)+'.csv') #download csv file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEo7H6jpOm8J"
      },
      "source": [
        "print(paths_all_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZj-aQyBOSMg"
      },
      "source": [
        "for j in conf_matrix_data:\n",
        "    plot_conf_matrix(j[0], j[1], j[2])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}